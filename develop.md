# AI-Assisted Development Log for Linear Regression Visualizer

æœ¬æ–‡ä»¶è¨˜éŒ„äº†ä½¿ç”¨ AI åŠ©ç†å”åŒé–‹ç™¼ã€Œäº’å‹•å¼ç·šæ€§è¿´æ­¸è¦–è¦ºåŒ–å·¥å…·ã€çš„å®Œæ•´éç¨‹ã€‚é–‹ç™¼æµç¨‹éµå¾ª `Todo.md` çš„è¦åŠƒï¼Œä¸¦ä»¥ `README.md` çš„å°ˆæ¡ˆç›®æ¨™ç‚ºæŒ‡å°ã€‚

## éšæ®µä¸€ï¼šå°ˆæ¡ˆå•Ÿå‹•èˆ‡æ–¹å‘ç¢ºèª

æ­¤éšæ®µçš„ç›®æ¨™æ˜¯ç¢ºç«‹æœ€çµ‚è¦äº¤ä»˜çš„æˆå“æ¨£è²Œï¼Œä¸¦è¦åŠƒé–‹ç™¼è·¯å¾‘ã€‚

### ğŸ‘¤ ä½¿ç”¨è€…åˆå§‹è«‹æ±‚

> è«‹æ ¹æ“šé€™å…©ä»½æ–‡ä»¶ (`Todo.md` å’Œ `README.md`) å¹«æˆ‘å®Œæˆæ‰€éœ€çš„ç¨‹å¼ç¢¼ï¼Œæˆ‘å€‘å¯ä»¥ä¸€æ­¥ä¸€æ­¥å°‡ç´°ç¯€è™•ç†å¥½ï¼Œæˆ‘å€‘å…ˆç¢ºèªæœ€å¾Œè¦äº¤å‡ºçš„æˆå“ï¼Œç„¶å¾Œå†é–‹å§‹ä½œæ¥­ã€‚

### ğŸ¤– AI å›æ‡‰èˆ‡é–‹ç™¼æ–¹å‘ç¢ºèª

AI åˆ†æäº†æ–‡ä»¶å¾Œï¼Œç¸½çµå‡ºå°ˆæ¡ˆçš„æ ¸å¿ƒç›®æ¨™ï¼š

1.  å»ºç«‹ä¸€å€‹ Streamlit æ‡‰ç”¨ç¨‹å¼ (`app.py`)ã€‚
2.  æ‡‰ç”¨ç¨‹å¼çš„æ ¸å¿ƒåŠŸèƒ½æ˜¯äº’å‹•å¼åœ°å±•ç¤ºç·šæ€§è¿´æ­¸ã€‚
3.  ç·šæ€§è¿´æ­¸æ¨¡å‹å¿…é ˆ**å¾é›¶é–‹å§‹å¯¦ä½œ**ï¼Œè€Œéä½¿ç”¨ç¾æˆå‡½å¼åº«ã€‚

AI æå‡ºäº†ä»¥ä¸‹é–‹ç™¼è—åœ–ï¼š
> æˆ‘å€‘ä¸»è¦å°ˆæ³¨æ–¼æ’°å¯«ä¸€å€‹ `linear_regression.py` æª”æ¡ˆï¼Œå…¶ä¸­åŒ…å«å¾é›¶é–‹å§‹å¯¦ä½œçš„ç·šæ€§è¿´æ­¸é‚è¼¯ï¼Œç„¶å¾Œå†å°‡é€™å€‹æ¨¡å‹æ•´åˆåˆ°ä¸€å€‹ `app.py`ï¼ˆStreamlit æ‡‰ç”¨ç¨‹å¼ï¼‰ä¸­ä¾†é€²è¡Œè¦–è¦ºåŒ–ã€‚

### âœ… æ–¹å‘ç¢ºèª

ä½¿ç”¨è€…åŒæ„äº†æ­¤é–‹ç™¼æ–¹å‘ï¼Œå°ˆæ¡ˆæ­£å¼é€²å…¥å¯¦ä½œéšæ®µã€‚

---

## éšæ®µäºŒï¼šæ•¸æ“šæº–å‚™æ¨¡çµ„ (`data_utils.py`)

æ­¤éšæ®µçš„ç›®æ¨™æ˜¯å®Œæˆ `Todo.md` ä¸­çš„ã€Œ1. Data Preparationã€ã€‚

### ğŸ‘¤ ä½¿ç”¨è€…è«‹æ±‚

> æ²’å•é¡Œï¼Œæˆ‘ä¹Ÿèªç‚ºæ˜¯é€™å€‹æ–¹å‘ï¼Œè®“æˆ‘å€‘é–‹å§‹ç¬¬ä¸€æ­¥å§ã€‚

### ğŸ¤– AI å›æ‡‰èˆ‡ç¨‹å¼ç¢¼å¯¦ä½œ

æ ¹æ“šå°ˆæ¡ˆçš„äº’å‹•æ€§éœ€æ±‚ï¼ŒAI å»ºè­°ç·¨å¯«ä¸€å€‹èƒ½å‹•æ…‹ç”Ÿæˆæ•¸æ“šçš„å‡½æ•¸ï¼Œè€Œéè®€å–å›ºå®šæª”æ¡ˆã€‚æ­¤å‡½æ•¸åŒ…å«äº†æ•¸æ“šç”Ÿæˆã€åˆ‡åˆ†èˆ‡ç‰¹å¾µç¸®æ”¾çš„å®Œæ•´æµç¨‹ã€‚

#### åˆç‰ˆç¨‹å¼ç¢¼

```python
# data_utils.py

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

def generate_and_prepare_data(n_samples=100, slope=2, intercept=5, noise_level=20, test_size=0.2, random_state=42):
    """
    ç”Ÿæˆã€åˆ‡åˆ†ä¸¦æ¨™æº–åŒ–ç”¨æ–¼ç·šæ€§è¿´æ­¸çš„æ•¸æ“šã€‚

    Args:
        n_samples (int): è¦ç”Ÿæˆçš„æ•¸æ“šé»æ•¸é‡ã€‚
        slope (float): ç·šæ€§é—œä¿‚çš„æ–œç‡ (w)ã€‚
        intercept (float): ç·šæ€§é—œä¿‚çš„æˆªè· (b)ã€‚
        noise_level (float): æ·»åŠ åˆ°æ•¸æ“šä¸­çš„éš¨æ©Ÿé›œè¨Šç­‰ç´šã€‚
        test_size (float): æ¸¬è©¦é›†æ‰€ä½”çš„æ¯”ä¾‹ã€‚
        random_state (int): ç”¨æ–¼ç¢ºä¿çµæœå¯é‡ç¾çš„éš¨æ©Ÿç¨®å­ã€‚

    Returns:
        tuple: åŒ…å« X_train_scaled, X_test_scaled, y_train, y_test, scaler ç‰©ä»¶ã€‚
    """
    # æ ¹æ“šç·šæ€§æ–¹ç¨‹å¼ y = wX + b ç”Ÿæˆæ•¸æ“šï¼Œä¸¦åŠ å…¥é›œè¨Š
    np.random.seed(random_state)
    X = 100 * (np.random.rand(n_samples, 1) - 0.5)
    noise = np.random.randn(n_samples, 1) * noise_level
    y = intercept + slope * X + noise

    # åˆ‡åˆ†è¨“ç·´é›†èˆ‡æ¸¬è©¦é›†
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)

    # ç‰¹å¾µç¸®æ”¾ (æ¨™æº–åŒ–)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    return X_train_scaled, X_test_scaled, y_train, y_test, scaler
```

### ğŸ’¡ é—œéµæ¶æ§‹æ±ºç­–ï¼šç¨‹å¼ç¢¼æ¨¡çµ„åŒ–

> **ğŸ‘¤ ä½¿ç”¨è€…æå•ï¼š** æˆ‘ç¢ºèªéäº†ï¼Œä½ è¦ºå¾—è¦æ‹†åˆ†æˆä¸åŒçš„ py file æœƒæ›´å¥½å—?

AI å¼·çƒˆå»ºè­°å°‡ç¨‹å¼ç¢¼æ‹†åˆ†åˆ°ä¸åŒæª”æ¡ˆä¸­ï¼Œç†ç”±å¦‚ä¸‹ï¼š
1.  **é—œæ³¨é»åˆ†é›¢ (Separation of Concerns)**ï¼š`data_utils.py`, `linear_regression.py`, `app.py` å„å¸å…¶è·ã€‚
2.  **å¯è®€æ€§èˆ‡å¯ç¶­è­·æ€§**ï¼šæ˜“æ–¼å°‹æ‰¾ã€ä¿®æ”¹å’Œé™¤éŒ¯ã€‚
3.  **å¯é‡ç”¨æ€§**ï¼šæ ¸å¿ƒæ¨¡å‹å¯è¢«å…¶ä»–å°ˆæ¡ˆè¼•æ˜“å¼•ç”¨ã€‚

æœ€çµ‚ç¢ºç«‹äº†å°ˆæ¡ˆçµæ§‹ï¼š
linear-regression-visualizer/
â”œâ”€â”€ app.py
â”œâ”€â”€ data_utils.py
â”œâ”€â”€ linear_regression.py
â””â”€â”€ requirements.txt


#### æœ€çµ‚æª”æ¡ˆ: `data_utils.py`
```python
# data_utils.py

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

def generate_and_prepare_data(n_samples=100, slope=2, intercept=5, noise_level=20, test_size=0.2, random_state=42):
    """
    ç”Ÿæˆã€åˆ‡åˆ†ä¸¦æ¨™æº–åŒ–ç”¨æ–¼ç·šæ€§è¿´æ­¸çš„æ•¸æ“šã€‚

    Args:
        n_samples (int): è¦ç”Ÿæˆçš„æ•¸æ“šé»æ•¸é‡ã€‚
        slope (float): ç·šæ€§é—œä¿‚çš„æ–œç‡ (w)ã€‚
        intercept (float): ç·šæ€§é—œä¿‚çš„æˆªè· (b)ã€‚
        noise_level (float): æ·»åŠ åˆ°æ•¸æ“šä¸­çš„éš¨æ©Ÿé›œè¨Šç­‰ç´šã€‚
        test_size (float): æ¸¬è©¦é›†æ‰€ä½”çš„æ¯”ä¾‹ã€‚
        random_state (int): ç”¨æ–¼ç¢ºä¿çµæœå¯é‡ç¾çš„éš¨æ©Ÿç¨®å­ã€‚

    Returns:
        tuple: åŒ…å« X_train_scaled, X_test_scaled, y_train, y_test, scaler ç‰©ä»¶ã€‚
    """
    # æ ¹æ“šç·šæ€§æ–¹ç¨‹å¼ y = wX + b ç”Ÿæˆæ•¸æ“šï¼Œä¸¦åŠ å…¥é›œè¨Š
    np.random.seed(random_state)
    X = 100 * (np.random.rand(n_samples, 1) - 0.5)
    noise = np.random.randn(n_samples, 1) * noise_level
    y = intercept + slope * X + noise

    # åˆ‡åˆ†è¨“ç·´é›†èˆ‡æ¸¬è©¦é›†
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)

    # ç‰¹å¾µç¸®æ”¾ (æ¨™æº–åŒ–)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    return X_train_scaled, X_test_scaled, y_train, y_test, scaler
```
---

## éšæ®µä¸‰ï¼šæ ¸å¿ƒæ¨¡å‹å¯¦ä½œ (`linear_regression.py`)

æ­¤éšæ®µçš„ç›®æ¨™æ˜¯å®Œæˆ `Todo.md` ä¸­çš„ã€Œ2. Model Implementationã€ï¼Œå¾é›¶é–‹å§‹æ‰“é€ ç·šæ€§è¿´æ­¸æ¨¡å‹ã€‚

### ğŸ‘¤ ä½¿ç”¨è€…è«‹æ±‚

> è«‹ç¹¼çºŒå§ã€‚

### ğŸ¤– AI å›æ‡‰èˆ‡ç¨‹å¼ç¢¼å¯¦ä½œ

AI æä¾›äº†ä¸€å€‹ `LinearRegression` é¡åˆ¥ï¼Œè©²é¡åˆ¥å°è£äº†æ‰€æœ‰æ¨¡å‹ç›¸é—œçš„é‚è¼¯ï¼ŒåŒ…æ‹¬ï¼š
-   æ¬Šé‡èˆ‡åç½®çš„åˆå§‹åŒ–
-   å‡è¨­å‡½æ•¸ (Hypothesis)
-   æˆæœ¬å‡½æ•¸ (Cost Function - MSE)
-   æ¢¯åº¦ä¸‹é™ (Gradient Descent) çš„å®Œæ•´å„ªåŒ–æµç¨‹

#### æœ€çµ‚æª”æ¡ˆ: `linear_regression.py`
```python
# linear_regression.py

import numpy as np


class LinearRegression:
    """
    å¾é›¶é–‹å§‹å¯¦ä½œçš„ç·šæ€§è¿´æ­¸æ¨¡å‹ã€‚

    ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•é€²è¡Œå„ªåŒ–ã€‚
    """

    def __init__(self, learning_rate=0.01, n_iterations=1000):
        """
        åˆå§‹åŒ–æ¨¡å‹ã€‚

        Args:
            learning_rate (float): å­¸ç¿’ç‡ï¼Œæ§åˆ¶æ¯ä¸€æ­¥æ›´æ–°çš„å¤§å°ã€‚
            n_iterations (int): æ¢¯åº¦ä¸‹é™çš„è¿­ä»£æ¬¡æ•¸ã€‚
        """
        self.learning_rate = learning_rate
        self.n_iterations = n_iterations
        self.weights = None
        self.bias = None
        self.cost_history = []

    def _initialize_parameters(self, n_features):
        """æ­¥é©Ÿ 2.1: åˆå§‹åŒ–æ¬Šé‡å’Œåç½®"""
        self.weights = np.zeros((n_features, 1))
        self.bias = 0

    def _compute_hypothesis(self, X):
        """æ­¥é©Ÿ 2.2: å®šç¾©å‡è¨­å‡½æ•¸ h(x) = wX + b"""
        return np.dot(X, self.weights) + self.bias

    def _compute_cost(self, y, y_pred):
        """æ­¥é©Ÿ 2.3: å®šç¾©æˆæœ¬å‡½æ•¸ (Mean Squared Error)"""
        m = len(y)
        cost = (1 / (2 * m)) * np.sum((y_pred - y) ** 2)
        return cost

    def _gradient_descent(self, X, y, y_pred):
        """æ­¥é©Ÿ 2.4: å¯¦ä½œæ¢¯åº¦ä¸‹é™"""
        m = len(y)

        # æ­¥é©Ÿ 2.4.1: è¨ˆç®—æ¢¯åº¦
        dw = (1 / m) * np.dot(X.T, (y_pred - y))
        db = (1 / m) * np.sum(y_pred - y)

        # æ­¥é©Ÿ 2.4.2: æ›´æ–°æ¬Šé‡å’Œåç½®
        self.weights -= self.learning_rate * dw
        self.bias -= self.learning_rate * db

    def fit(self, X, y):
        """
        ä½¿ç”¨è¨“ç·´æ•¸æ“šä¾†è¨“ç·´æ¨¡å‹ã€‚

        Args:
            X (np.ndarray): è¨“ç·´æ•¸æ“šçš„ç‰¹å¾µï¼Œç¶­åº¦ç‚º (n_samples, n_features)ã€‚
            y (np.ndarray): è¨“ç·´æ•¸æ“šçš„ç›®æ¨™å€¼ï¼Œç¶­åº¦ç‚º (n_samples, 1)ã€‚
        """
        # å–å¾—æ¨£æœ¬æ•¸å’Œç‰¹å¾µæ•¸
        n_samples, n_features = X.shape

        # åˆå§‹åŒ–åƒæ•¸
        self._initialize_parameters(n_features)

        # é–‹å§‹æ¢¯åº¦ä¸‹é™è¿­ä»£
        for i in range(self.n_iterations):
            # 1. è¨ˆç®—é æ¸¬å€¼
            y_pred = self._compute_hypothesis(X)

            # 2. è¨ˆç®—æˆæœ¬å‡½æ•¸ï¼Œä¸¦è¨˜éŒ„ä¸‹ä¾†ä»¥ä¾›å¾ŒçºŒåˆ†æ
            cost = self._compute_cost(y, y_pred)
            self.cost_history.append(cost)

            # 3. åŸ·è¡Œæ¢¯åº¦ä¸‹é™ä¾†æ›´æ–°åƒæ•¸
            self._gradient_descent(X, y, y_pred)

            # (å¯é¸) æ¯éš”ä¸€å®šæ¬¡æ•¸æ‰“å°ä¸€æ¬¡æˆæœ¬ï¼Œæ–¹ä¾¿ç›£æ§
            # if (i % 100) == 0:
            #     print(f"Iteration {i}: Cost {cost:.4f}")

    def predict(self, X):
        """
        ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹é€²è¡Œé æ¸¬ã€‚

        Args:
            X (np.ndarray): è¦é æ¸¬çš„æ•¸æ“šï¼Œç¶­åº¦ç‚º (n_samples, n_features)ã€‚

        Returns:
            np.ndarray: é æ¸¬çµæœã€‚
        """
        if self.weights is None or self.bias is None:
            raise Exception("æ¨¡å‹å°šæœªè¨“ç·´ï¼Œè«‹å…ˆèª¿ç”¨ fit() æ–¹æ³•ã€‚")

        return self._compute_hypothesis(X)


# --- ä½¿ç”¨ç¯„ä¾‹ (ç”¨æ–¼ç¨ç«‹æ¸¬è©¦æ­¤æ¨¡çµ„) ---
if __name__ == "__main__":
    # å¼•å…¥æˆ‘å€‘å‰›å¯«å¥½çš„æ•¸æ“šè™•ç†å·¥å…·
    from data_utils import generate_and_prepare_data

    # 1. æº–å‚™æ•¸æ“š
    X_train, X_test, y_train, y_test, _ = generate_and_prepare_data()

    # 2. å»ºç«‹ä¸¦è¨“ç·´æ¨¡å‹
    model = LinearRegression(learning_rate=0.01, n_iterations=1000)
    model.fit(X_train, y_train)

    # 3. é€²è¡Œé æ¸¬
    predictions = model.predict(X_test)

    # 4. æ‰“å°çµæœ
    print("æ¨¡å‹è¨“ç·´å®Œæˆã€‚")
    print(f"å­¸ç¿’åˆ°çš„æ¬Šé‡ (w): {model.weights[0][0]:.4f}")
    print(f"å­¸ç¿’åˆ°çš„åç½® (b): {model.bias:.4f}")

    # ç°¡å–®æ¯”è¼ƒä¸€ä¸‹å‰ 5 å€‹é æ¸¬å€¼å’ŒçœŸå¯¦å€¼
    print("\n--- é æ¸¬ vs. çœŸå¯¦ ---")
    for i in range(5):
        print(f"é æ¸¬å€¼: {predictions[i][0]:.2f}, çœŸå¯¦å€¼: {y_test[i][0]:.2f}")

```
---

## éšæ®µå››ï¼šStreamlit ä»‹é¢èˆ‡æ•´åˆ (`app.py`)

æ­¤éšæ®µçš„ç›®æ¨™æ˜¯å®Œæˆ `Todo.md` ä¸­å‰©é¤˜çš„ã€ŒTrainingã€ã€ã€ŒEvaluationã€ã€ã€ŒPredictionã€å’Œã€ŒReportingã€éƒ¨åˆ†ï¼Œä¸¦å°‡æ‰€æœ‰æ¨¡çµ„æ•´åˆåˆ°ä¸€å€‹äº’å‹•å¼ Web æ‡‰ç”¨ä¸­ã€‚

### ğŸ‘¤ ä½¿ç”¨è€…è«‹æ±‚

> æ²’å•é¡Œï¼Œè«‹å¹«æˆ‘å®Œæˆé€™éƒ¨åˆ†ã€‚

### ğŸ¤– AI å›æ‡‰èˆ‡ç¨‹å¼ç¢¼å¯¦ä½œ

AI æä¾›äº† `app.py` çš„å®Œæ•´ç¨‹å¼ç¢¼ï¼Œå…¶ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š
1.  **Streamlit ä»‹é¢**ï¼šä½¿ç”¨å´é‚Šæ¬„è®“ä½¿ç”¨è€…èª¿æ•´æ•¸æ“šå’Œæ¨¡å‹åƒæ•¸ã€‚
2.  **æ¨¡çµ„æ•´åˆ**ï¼šå°å…¥ä¸¦ä½¿ç”¨ `data_utils.py` å’Œ `linear_regression.py`ã€‚
3.  **æ¨¡å‹è¨“ç·´èˆ‡è©•ä¼°**ï¼šæ ¹æ“šä½¿ç”¨è€…è¼¸å…¥å³æ™‚è¨“ç·´æ¨¡å‹ï¼Œä¸¦è¨ˆç®— MSE å’Œ RÂ² åˆ†æ•¸ã€‚
4.  **çµæœè¦–è¦ºåŒ–**ï¼šç¹ªè£½æˆæœ¬å‡½æ•¸æ”¶æ–‚æ›²ç·šåœ–å’Œé æ¸¬çµæœå°æ¯”åœ–ã€‚
5.  **èªªæ˜èˆ‡å ±å‘Š**ï¼šåœ¨ä»‹é¢ä¸Šæä¾› Markdown æ ¼å¼çš„åœ–è¡¨èªªæ˜èˆ‡çµè«–ã€‚

#### æœ€çµ‚æª”æ¡ˆ: `app.py`
```python
# app.py

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import streamlit as st

# å¼•å…¥æˆ‘å€‘è‡ªå®šç¾©çš„æ¨¡çµ„
from data_utils import generate_and_prepare_data
from linear_regression import LinearRegression


# --- è¼”åŠ©å‡½æ•¸ (ç”¨æ–¼è¨ˆç®—è©•ä¼°æŒ‡æ¨™) ---
def mean_squared_error(y_true, y_pred):
    """è¨ˆç®—å‡æ–¹èª¤å·® (MSE)"""
    return np.mean((y_true - y_pred) ** 2)


def r2_score(y_true, y_pred):
    """è¨ˆç®— R-squared (R2 Score)"""
    corr_matrix = np.corrcoef(y_true.flatten(), y_pred.flatten())
    corr = corr_matrix[0, 1]
    return corr**2


# --- Streamlit æ‡‰ç”¨ç¨‹å¼ä»‹é¢ ---

st.set_page_config(layout="wide")

# 1. æ¨™é¡Œ
st.title("ğŸ“ˆ äº’å‹•å¼ç·šæ€§è¿´æ­¸è¦–è¦ºåŒ–å·¥å…·")
st.markdown(
    "é€™å€‹å·¥å…·è®“ä½ å¾é›¶é–‹å§‹æ¢ç´¢ç·šæ€§è¿´æ­¸ã€‚ä½ å¯ä»¥åœ¨å·¦å´çš„å´é‚Šæ¬„èª¿æ•´åƒæ•¸ï¼Œè§€å¯Ÿæ¨¡å‹çš„è®ŠåŒ–ã€‚"
)

# 2. å´é‚Šæ¬„ï¼šåƒæ•¸æ§åˆ¶
st.sidebar.header("âš™ï¸ åƒæ•¸è¨­å®š")

st.sidebar.subheader("ğŸ“Š æ•¸æ“šç”Ÿæˆåƒæ•¸")
n_samples = st.sidebar.slider("æ•¸æ“šé»æ•¸é‡ (N)", 50, 500, 100, 10)
slope = st.sidebar.slider("çœŸå¯¦æ–œç‡ (w)", -5.0, 5.0, 2.0, 0.1)
intercept = st.sidebar.slider("çœŸå¯¦æˆªè· (b)", -10.0, 10.0, 5.0, 0.5)
noise_level = st.sidebar.slider("é›œè¨Šç­‰ç´š", 0.0, 50.0, 20.0, 1.0)

st.sidebar.subheader("ğŸ§  æ¨¡å‹è¨“ç·´åƒæ•¸")
learning_rate = st.sidebar.select_slider(
    "å­¸ç¿’ç‡ (Learning Rate)", options=[0.0001, 0.001, 0.01, 0.1, 1.0], value=0.01
)
n_iterations = st.sidebar.slider("è¿­ä»£æ¬¡æ•¸", 100, 3000, 1000, 100)

# 3. æ•¸æ“šæº–å‚™èˆ‡æ¨¡å‹è¨“ç·´
# æ ¹æ“šå´é‚Šæ¬„çš„åƒæ•¸ç”Ÿæˆæ•¸æ“š
X_train, X_test, y_train, y_test, scaler = generate_and_prepare_data(
    n_samples=n_samples, slope=slope, intercept=intercept, noise_level=noise_level
)

# å»ºç«‹ä¸¦è¨“ç·´æ¨¡å‹
model = LinearRegression(learning_rate=learning_rate, n_iterations=n_iterations)
model.fit(X_train, y_train)

# é€²è¡Œé æ¸¬
y_pred_test = model.predict(X_test)

# 4. é¡¯ç¤ºçµæœ
st.header("âœ¨ çµæœèˆ‡åˆ†æ")

col1, col2 = st.columns((1, 1))

with col1:
    # æ­¥é©Ÿ 3: ç›£æ§æ”¶æ–‚éç¨‹
    st.subheader("ğŸ“‰ æˆæœ¬å‡½æ•¸æ”¶æ–‚éç¨‹")

    fig_cost, ax_cost = plt.subplots()
    ax_cost.plot(range(model.n_iterations), model.cost_history)
    ax_cost.set_xlabel("è¿­ä»£æ¬¡æ•¸ (Iterations)")
    ax_cost.set_ylabel("æˆæœ¬ (Cost - MSE)")
    ax_cost.set_title("Cost Function over Iterations")
    sns.despine(fig=fig_cost)
    st.pyplot(fig_cost)

    st.markdown("""
    ä¸Šåœ–é¡¯ç¤ºäº†éš¨è‘—è¨“ç·´çš„é€²è¡Œï¼Œæ¨¡å‹çš„æˆæœ¬ï¼ˆèª¤å·®ï¼‰å¦‚ä½•é€æ¼¸é™ä½ã€‚
    ä¸€å€‹ç†æƒ³çš„å­¸ç¿’ç‡æœƒè®“é€™æ¢æ›²ç·šå¹³æ»‘åœ°ä¸‹é™ä¸¦æ”¶æ–‚ã€‚
    - å¦‚æœæ›²ç·šä¸‹é™å¤ªæ…¢ï¼Œå¯ä»¥å˜—è©¦**æé«˜å­¸ç¿’ç‡**ã€‚
    - å¦‚æœæ›²ç·šåŠ‡çƒˆéœ‡ç›ªæˆ–ç™¼æ•£ï¼Œè¡¨ç¤ºå­¸ç¿’ç‡å¤ªé«˜ï¼Œéœ€è¦**é™ä½å­¸ç¿’ç‡**ã€‚
    """)

with col2:
    # æ­¥é©Ÿ 4: è©•ä¼°èˆ‡è¦–è¦ºåŒ–é æ¸¬
    st.subheader("ğŸ¯ é æ¸¬çµæœè¦–è¦ºåŒ–")

    fig_pred, ax_pred = plt.subplots()
    # åŸå§‹æ•¸æ“šé» (æ¸¬è©¦é›†)
    ax_pred.scatter(
        scaler.inverse_transform(X_test),
        y_test,
        alpha=0.7,
        label="çœŸå¯¦å€¼ (Actual Values)",
    )
    # è¿´æ­¸ç·š
    ax_pred.plot(
        scaler.inverse_transform(X_test),
        y_pred_test,
        color="red",
        linewidth=2,
        label="é æ¸¬ç·š (Prediction)",
    )
    ax_pred.set_xlabel("ç‰¹å¾µ (Feature X)")
    ax_pred.set_ylabel("ç›®æ¨™ (Target y)")
    ax_pred.set_title("Prediction vs. Actual Values")
    ax_pred.legend()
    sns.despine(fig=fig_pred)
    st.pyplot(fig_pred)

    st.markdown("""
    ä¸Šåœ–å±•ç¤ºäº†æ¨¡å‹åœ¨**æœªè¦‹éçš„æ¸¬è©¦æ•¸æ“š**ä¸Šçš„è¡¨ç¾ã€‚
    - **è—é»**æ˜¯çœŸå¯¦çš„æ•¸æ“šåˆ†ä½ˆã€‚
    - **ç´…ç·š**æ˜¯æˆ‘å€‘çš„æ¨¡å‹å­¸ç¿’åˆ°çš„ç·šæ€§é—œä¿‚ã€‚
    ç´…ç·šè¶Šèƒ½è²¼è¿‘è—é»çš„åˆ†ä½ˆè¶¨å‹¢ï¼Œä»£è¡¨æ¨¡å‹å­¸å¾—è¶Šå¥½ã€‚
    """)

# æ­¥é©Ÿ 4: è¨ˆç®—è©•ä¼°æŒ‡æ¨™
st.subheader("ğŸ“ æ¨¡å‹è©•ä¼°æŒ‡æ¨™")
mse = mean_squared_error(y_test, y_pred_test)
r2 = r2_score(y_test, y_pred_test)

metric1, metric2, metric3, metric4 = st.columns(4)
metric1.metric(label="å‡æ–¹èª¤å·® (MSE)", value=f"{mse:.2f}")
metric2.metric(label="R-squared (RÂ² Score)", value=f"{r2:.4f}")

st.info(
    f"æ¨¡å‹å­¸ç¿’åˆ°çš„æ¬Šé‡ (w): **{model.weights[0][0]:.4f}** | å­¸ç¿’åˆ°çš„åç½® (b): **{model.bias:.4f}**",
    icon="ğŸ§ ",
)


st.header("ğŸ“œ çµè«–")
st.markdown("""
é€™å€‹äº’å‹•å·¥å…·å±•ç¤ºäº†ç·šæ€§è¿´æ­¸çš„æ ¸å¿ƒæµç¨‹ã€‚é€éèª¿æ•´å·¦å´çš„åƒæ•¸ï¼Œæˆ‘å€‘å¯ä»¥è§€å¯Ÿåˆ°ï¼š
1.  **æ•¸æ“šç‰¹æ€§**ï¼šå¢åŠ  `é›œè¨Šç­‰ç´š` æœƒè®“æ•¸æ“šé»æ›´åˆ†æ•£ï¼Œæ¨¡å‹æ›´é›£æ‰¾åˆ°æœ€ä½³æ“¬åˆç·šï¼Œå°è‡´ RÂ² åˆ†æ•¸ä¸‹é™ã€‚
2.  **æ¨¡å‹è¨“ç·´**ï¼š`å­¸ç¿’ç‡` å’Œ `è¿­ä»£æ¬¡æ•¸` ç›´æ¥å½±éŸ¿æ¨¡å‹çš„æ”¶æ–‚æ•ˆæœã€‚ä¸æ°ç•¶çš„å­¸ç¿’ç‡æœƒå°è‡´æˆæœ¬ç„¡æ³•æœ‰æ•ˆé™ä½ã€‚
3.  **å¾é›¶å¯¦ä½œ**ï¼šæˆ‘å€‘åº•å±¤ä½¿ç”¨çš„ `LinearRegression` é¡åˆ¥æ˜¯å®Œå…¨å¾é›¶æ‰“é€ çš„ï¼ŒæˆåŠŸå¯¦ç¾äº†æ¢¯åº¦ä¸‹é™çš„å„ªåŒ–éç¨‹ã€‚

æˆ‘å€‘å·²ç¶“æˆåŠŸå®Œæˆäº† `Todo.md` ä¸­å®šç¾©çš„æ‰€æœ‰ä¸»è¦æ­¥é©Ÿï¼
""")

```

---
## çµè«–

é€éèˆ‡ AI åŠ©ç†çš„é€æ­¥å”ä½œï¼Œæˆ‘å€‘æˆåŠŸåœ°æŒ‰ç…§ `Todo.md` çš„è¦åŠƒï¼Œå®Œæˆäº†ä¸€å€‹åŠŸèƒ½å®Œæ•´çš„äº’å‹•å¼ç·šæ€§è¿´æ­¸å°ˆæ¡ˆã€‚æ•´å€‹éç¨‹æ¶µè“‹äº†å¾éœ€æ±‚ç¢ºèªã€æ¶æ§‹è¨­è¨ˆã€æ¨¡çµ„åŒ–ç·¨ç¢¼åˆ°æœ€çµ‚æ•´åˆèˆ‡å‘ˆç¾çš„å®Œæ•´è»Ÿé«”é–‹ç™¼æµç¨‹ã€‚